{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumilhooda/MLDLNNtoCV/blob/main/Q4_NNFL_Assignment1_SoumilHooda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDJMdC_oy6PW",
        "outputId": "c4e4586d-9ddf-467b-bf3c-6d048e4889ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO36laxHZjf5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import sklearn.metrics as skm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT8Of-ZPehRD"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return 1.0/(1+np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V_P1Nh1eltG"
      },
      "outputs": [],
      "source": [
        "def cost_function_LOR(X,y,w):\n",
        "  hypothesis = sigmoid(np.dot(X,w.T))\n",
        "  J = -(1/m)*(np.sum((y.T)*(np.log(hypothesis))+((1-y).T)*np.log(1-hypothesis)))\n",
        "  return J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjLHZ0JDeovs"
      },
      "outputs": [],
      "source": [
        "def cost_function_LOR_L2(X,y,w,lamb):\n",
        "  hypothesis = sigmoid(np.dot(X,w.T))\n",
        "  J = -(1/m)*(np.sum((y.T)*(np.log(hypothesis))+((1-y).T)*np.log(1-hypothesis))) + (lamb/2)*(np.sum(np.square(w)))\n",
        "  return J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDooGi1yepkT"
      },
      "outputs": [],
      "source": [
        "def cost_function_LOR_L1(X,y,w,lamb):\n",
        "  hypothesis = sigmoid(np.dot(X,w.T))\n",
        "  J = -(1/m)*(np.sum((y.T)*(np.log(hypothesis))+((1-y).T)*np.log(1-hypothesis)))+(lamb/2)*np.sum(np.abs(w))\n",
        "  return J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hhj76VLxerMc"
      },
      "outputs": [],
      "source": [
        "def batch_gradient_descent_LOR(X,y,w,alpha,iters):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    hypothesis = sigmoid(np.dot(X,w.T))\n",
        "    w = w-(alpha/len(y))*np.dot((hypothesis-y).T,X)\n",
        "    cost_history[i] = cost_function_LOR(X,y,w)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51akf8n2euMM"
      },
      "outputs": [],
      "source": [
        "def batch_gradient_descent_LOR_L2(X,y,w,alpha,iters,lamb):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    hypothesis = sigmoid(np.dot(X,w.T))\n",
        "    w = w*(1-(alpha*lamb))-(alpha/len(y))*np.dot((hypothesis-y).T,X)\n",
        "    cost_history[i] = cost_function_LOR_L2(X,y,w,lamb)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q1640RheuqE"
      },
      "outputs": [],
      "source": [
        "def batch_gradient_descent_LOR_L1(X,y,w,alpha,iters,lamb):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    hypothesis = sigmoid(np.dot(X,w.T))\n",
        "    w = w-(((alpha*lamb)/2)*np.sign(w))-(alpha/len(y))*np.dot((hypothesis-y).T,X)\n",
        "    cost_history[i] = cost_function_LOR_L1(X,y,w,lamb)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYZwVGiZewZG"
      },
      "outputs": [],
      "source": [
        "def MB_gradient_descent_LOR(X,y,w,alpha,iters,batch_size):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    rand_index = np.random.randint(len(y)-batch_size)\n",
        "    ind_x = X[rand_index:rand_index+batch_size]\n",
        "    ind_y = y[rand_index:rand_index+batch_size]\n",
        "    w = w - (alpha/batch_size)*((ind_x.T.dot(sigmoid(ind_x.dot(w.T))-ind_y)).T)\n",
        "    cost_history[i] = cost_function_LOR(ind_x,ind_y,w)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBj8jY2WeyUB"
      },
      "outputs": [],
      "source": [
        "def MB_gradient_descent_LOR_L2(X,y,w,alpha,iters,batch_size,lamb):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    rand_index = np.random.randint(len(y)-batch_size)\n",
        "    ind_x = X[rand_index:rand_index+batch_size]\n",
        "    ind_y = y[rand_index:rand_index+batch_size]\n",
        "    w = w*(1-alpha*lamb) - (alpha/batch_size)*((ind_x.T.dot(sigmoid(ind_x.dot(w.T))-ind_y)).T)\n",
        "    cost_history[i] = cost_function_LOR_L2(ind_x,ind_y,w,lamb)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LejR0aPcez7g"
      },
      "outputs": [],
      "source": [
        "def MB_gradient_descent_LOR_L1(X,y,w,alpha,iters,batch_size,lamb):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    rand_index = np.random.randint(len(y)-batch_size)\n",
        "    ind_x = X[rand_index:rand_index+batch_size]\n",
        "    ind_y = y[rand_index:rand_index+batch_size]\n",
        "    w = w-(((alpha*lamb)/2)*np.sign(w)) - (alpha/batch_size)*((ind_x.T.dot(sigmoid(ind_x.dot(w.T))-ind_y)).T)\n",
        "    cost_history[i] = cost_function_LOR_L1(ind_x,ind_y,w,lamb)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQLZl84Ee1vk"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent_LOR(X,y,w,alpha,iters):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    rand_index = np.random.randint(len(y)-1)\n",
        "    ind_x = X[rand_index:rand_index+1]\n",
        "    ind_y = y[rand_index:rand_index+1]\n",
        "    w = w - alpha*((ind_x.T.dot(sigmoid(ind_x.dot(w.T))-ind_y)).T)\n",
        "    cost_history[i] = cost_function_LOR(ind_x,ind_y,w)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sD5XxXke3QM"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent_LOR_L2(X,y,w,alpha,iters,lamb):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    rand_index = np.random.randint(len(y)-1)\n",
        "    ind_x = X[rand_index:rand_index+1]\n",
        "    ind_y = y[rand_index:rand_index+1]\n",
        "    w = w*(1-alpha*lamb) - alpha*((ind_x.T.dot(sigmoid(ind_x.dot(w.T))-ind_y)).T)\n",
        "    cost_history[i] = cost_function_LOR_L2(ind_x,ind_y,w,lamb)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOxC_zRfe47P"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent_LOR_L1(X,y,w,alpha,iters,lamb):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    rand_index = np.random.randint(len(y)-1)\n",
        "    ind_x = X[rand_index:rand_index+1]\n",
        "    ind_y = y[rand_index:rand_index+1]\n",
        "    w = w-(((alpha*lamb)/2)*np.sign(w)) - alpha*((ind_x.T.dot(sigmoid(ind_x.dot(w.T))-ind_y)).T)\n",
        "    cost_history[i] = cost_function_LOR_L1(ind_x,ind_y,w,lamb)\n",
        "  return w, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SclO9ZB7e_EC"
      },
      "outputs": [],
      "source": [
        "def LOR_BGD(alpha,iters):\n",
        "  LOR_BGD_weight, LOR_BGD_J = batch_gradient_descent_LOR(Xtrain, Ytr, w, alpha, iters)\n",
        "  plt.plot(range(iters),LOR_BGD_J)\n",
        "  Ypred_LOR_BGD = sigmoid(np.dot(Xvalid,LOR_BGD_weight.T))\n",
        "  return Ypred_LOR_BGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syn8tsp5e_cn"
      },
      "outputs": [],
      "source": [
        "def LOR_MBGD(alpha,iters,batch_size):\n",
        "  LOR_MBGD_weight, LOR_MBGD_J = MB_gradient_descent_LOR(Xtrain,Ytr,w,alpha,iters,batch_size)\n",
        "  plt.plot(range(iters),LOR_MBGD_J)\n",
        "  Ypred_LOR_MBGD = sigmoid(np.dot(Xvalid,LOR_MBGD_weight.T))\n",
        "  return Ypred_LOR_MBGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q54aeuJMfBgc"
      },
      "outputs": [],
      "source": [
        "def LOR_SGD(alpha,iters):\n",
        "  LOR_SGD_weight, LOR_SGD_J = stochastic_gradient_descent_LOR(Xtrain,Ytr,w,alpha,iters)\n",
        "  Ypred_LOR_SGD = sigmoid(np.dot(Xvalid,LOR_SGD_weight.T))\n",
        "  return Ypred_LOR_SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7eXxY8SfQbd"
      },
      "outputs": [],
      "source": [
        "def LOR_L2_BGD(alpha,iters,lamb):\n",
        "  LOR_L2_BGD_weight, LOR_L2_BGD_J = batch_gradient_descent_LOR_L2(Xtrain, Ytr, w, alpha, iters, lamb)\n",
        "  Ypred_LOR_L2_BGD = sigmoid(np.dot(Xvalid,LOR_L2_BGD_weight.T))\n",
        "  return Ypred_LOR_L2_BGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpAQ-RFXfSSU"
      },
      "outputs": [],
      "source": [
        "def LOR_L2_MBGD(alpha,iters,batch_size,lamb):\n",
        "  LOR_L2_MBGD_weight, LOR_L2_MBGD_J = MB_gradient_descent_LOR_L2(Xtrain,Ytr,w,alpha,iters,batch_size,lamb)\n",
        "  Ypred_LOR_L2_MBGD = sigmoid(np.dot(Xvalid,LOR_L2_MBGD_weight.T))\n",
        "  return Ypred_LOR_L2_MBGD "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AOSmY_-fT3M"
      },
      "outputs": [],
      "source": [
        "def LOR_L2_SGD(alpha,iters,lamb):\n",
        "  LOR_L2_SGD_weight, LOR_L2_SGD_J = stochastic_gradient_descent_LOR_L2(Xtrain,Ytr,w,alpha,iters,lamb)\n",
        "  Ypred_LOR_L2_SGD = sigmoid(np.dot(Xvalid,LOR_L2_SGD_weight.T))\n",
        "  return Ypred_LOR_L2_SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAi7LrAvfWS7"
      },
      "outputs": [],
      "source": [
        "def LOR_L1_BGD(alpha,iters,lamb):\n",
        "  LOR_L1_BGD_weight, LOR_L1_BGD_J = batch_gradient_descent_LOR_L1(Xtrain,Ytr,w,alpha,iters,lamb)\n",
        "  Ypred_LOR_L1_BGD = sigmoid(np.dot(Xvalid,LOR_L1_BGD_weight.T))\n",
        "  return Ypred_LOR_L1_BGD "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0BWukwCfYDS"
      },
      "outputs": [],
      "source": [
        "def LOR_L1_MBGD(alpha,iters,batch_size,lamb):\n",
        "  LOR_L1_MBGD_weight, LOR_L1_MBGD_J = MB_gradient_descent_LOR_L1(Xtrain,Ytr,w,alpha,iters,batch_size,lamb)\n",
        "  Ypred_LOR_L1_MBGD = sigmoid(np.dot(Xvalid,LOR_L1_MBGD_weight.T))\n",
        "  return Ypred_LOR_L1_MBGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkVEwVPKfab7"
      },
      "outputs": [],
      "source": [
        "def LOR_L1_SGD(alpha,iters,lamb):\n",
        "  LOR_L1_SGD_weight, LOR_L1_SGD_J = stochastic_gradient_descent_LOR_L1(Xtrain,Ytr,w,alpha,iters,lamb)\n",
        "  Ypred_LOR_L1_SGD = sigmoid(np.dot(Xvalid,LOR_L1_SGD_weight.T))\n",
        "  return Ypred_LOR_L1_SGD\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQsmBk3NZm9F",
        "outputId": "da43dc03-b588-49fd-f38a-d6c459bb74ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              0           1           2           3           4           5   \\\n",
            "0     242.751526  281.801298  250.032405  132.099709   61.166502  247.837695   \n",
            "1     216.585951  297.057883  300.938478  131.358295  105.976730  273.299350   \n",
            "2     265.735536  339.271134  269.817305  102.304781   83.869539  281.962656   \n",
            "3     207.631953  255.284282  254.563071  229.883639   87.492384  303.314846   \n",
            "4     205.016124  333.265999  536.342842  106.237587  155.700409  272.692251   \n",
            "...          ...         ...         ...         ...         ...         ...   \n",
            "3407  723.913528  343.529660  360.468834  233.711682  177.309124  717.744261   \n",
            "3408  466.593370  215.858228  235.940729  309.475340  126.789443  333.123293   \n",
            "3409  446.227198  219.936910  181.605753  160.483773  106.179660  416.263221   \n",
            "3410  511.406437  215.379710  170.598957   89.543149   73.107090  457.783309   \n",
            "3411  757.967516  268.431243  189.755280  156.957408  126.298293  835.417941   \n",
            "\n",
            "              6           7           8           9   ...        51        52  \\\n",
            "0     306.999281  271.560155  126.038604   74.050379  ...  1.891884  1.612099   \n",
            "1     380.251154  335.864007  109.527577  113.088266  ...  1.779043  1.592470   \n",
            "2     386.806725  347.201912  105.945737  104.016513  ...  1.524681  1.763061   \n",
            "3     310.329644  276.926943  207.445146  108.620999  ...  1.907888  1.638839   \n",
            "4     367.533485  537.061350  110.167703  133.100524  ...  1.818213  1.860656   \n",
            "...          ...         ...         ...         ...  ...       ...       ...   \n",
            "3407  448.791348  379.113979  248.029948  170.735388  ...  1.928222  1.659960   \n",
            "3408  200.966375  173.674275  301.590225  110.481002  ...  1.841637  1.836036   \n",
            "3409  251.314023  196.936825  247.802048  114.649466  ...  1.953375  1.831422   \n",
            "3410  230.770136  178.846709  123.660090   78.208996  ...  1.897879  1.903539   \n",
            "3411  337.510327  247.097266  257.882395  180.132129  ...  1.906428  1.674688   \n",
            "\n",
            "            53        54        55        56        57        58        59  60  \n",
            "0     1.396853  1.437446  1.880098  1.872789  1.603211  1.281972  1.844860   1  \n",
            "1     1.749967  1.430117  1.935712  1.739076  1.572927  1.598582  1.625395   1  \n",
            "2     1.935089  1.642102  1.929802  1.522725  1.849940  1.848160  1.706134   1  \n",
            "3     1.604410  1.683284  1.845578  1.978957  1.682881  1.710404  1.664391   1  \n",
            "4     1.789285  1.606024  1.758654  1.834728  1.740251  1.703264  1.535684   1  \n",
            "...        ...       ...       ...       ...       ...       ...       ...  ..  \n",
            "3407  1.658291  1.640978  1.888330  1.933384  1.661284  1.948758  1.758541   4  \n",
            "3408  1.556465  1.898255  1.960397  1.973080  1.740866  1.568558  1.494537   4  \n",
            "3409  1.673341  1.889050  1.862514  2.004401  1.861006  1.418510  1.772295   4  \n",
            "3410  1.836154  1.841137  1.789531  1.837314  1.957492  1.609298  1.664890   4  \n",
            "3411  1.805059  1.529764  1.835415  2.056879  1.732298  1.790039  1.560633   4  \n",
            "\n",
            "[3412 rows x 61 columns]\n",
            "(3412, 61)\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_excel('/content/drive/MyDrive/data_Q4.xlsx',header=None)\n",
        "print(data)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zcxJZtppGSx",
        "outputId": "6cae5638-9807-4662-ebae-894480bb2aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[242.75152634 281.80129772 250.03240457 ...   1.28197209   1.8448603\n",
            "    1.        ]\n",
            " [216.58595112 297.05788313 300.9384782  ...   1.59858185   1.62539544\n",
            "    1.        ]\n",
            " [265.73553593 339.27113445 269.81730525 ...   1.84815977   1.70613365\n",
            "    1.        ]\n",
            " ...\n",
            " [446.22719796 219.93690991 181.60575345 ...   1.41851014   1.77229548\n",
            "    4.        ]\n",
            " [511.40643675 215.37971012 170.59895734 ...   1.60929772   1.66489041\n",
            "    4.        ]\n",
            " [757.96751558 268.43124317 189.75528024 ...   1.79003949   1.56063308\n",
            "    4.        ]]\n",
            "[[573.19870275 595.27566547 593.17166476 ...   1.61345894   1.48090695\n",
            "    4.        ]\n",
            " [231.51262931 215.23125237 254.65822393 ...   1.83690259   1.78019036\n",
            "    4.        ]\n",
            " [840.06737791 332.70763817 492.28377804 ...   1.79864128   1.66619011\n",
            "    4.        ]\n",
            " ...\n",
            " [154.88537008 158.11084583 193.21013838 ...   1.60882344   1.76936161\n",
            "    2.        ]\n",
            " [683.64324893 429.82830679 341.88011742 ...   1.66156869   1.3094515\n",
            "    4.        ]\n",
            " [145.73154701 186.94654145 112.61636092 ...   1.95425978   1.41555697\n",
            "    4.        ]]\n"
          ]
        }
      ],
      "source": [
        "datavalues = data.values\n",
        "print(datavalues)\n",
        "datavalues = np.take(datavalues,np.random.permutation(datavalues.shape[0]),axis=0,out=datavalues)\n",
        "print(datavalues)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8AYNJcnZ2GH",
        "outputId": "4b9ca475-d370-4fdf-bfee-27b6947c124a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[573.19870275 595.27566547 593.17166476 ...   1.67088304   1.61345894\n",
            "    1.48090695]\n",
            " [231.51262931 215.23125237 254.65822393 ...   1.63242537   1.83690259\n",
            "    1.78019036]\n",
            " [840.06737791 332.70763817 492.28377804 ...   1.63952237   1.79864128\n",
            "    1.66619011]\n",
            " ...\n",
            " [154.88537008 158.11084583 193.21013838 ...   1.70308942   1.60882344\n",
            "    1.76936161]\n",
            " [683.64324893 429.82830679 341.88011742 ...   1.72834611   1.66156869\n",
            "    1.3094515 ]\n",
            " [145.73154701 186.94654145 112.61636092 ...   1.88107471   1.95425978\n",
            "    1.41555697]]\n",
            "(3412, 60)\n"
          ]
        }
      ],
      "source": [
        "X = datavalues[:,:60]\n",
        "print(X)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmustXTAaHwe",
        "outputId": "3f8b5a2f-7b79-4951-bf33-4c58325ebed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4. 4. 4. ... 2. 4. 4.]\n",
            "(3412,)\n"
          ]
        }
      ],
      "source": [
        "Y = datavalues[:,60]\n",
        "print(Y)\n",
        "print(Y.shape) #output classes are of type 1,2,3 and 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cijWLni4buSO"
      },
      "outputs": [],
      "source": [
        "m = X.shape[0]\n",
        "xmin = np.min(X,axis=0)\n",
        "xmax = np.max(X, axis =0)\n",
        "X =(X-xmin)/(xmax-xmin)\n",
        "\n",
        "m = Y.shape[0]\n",
        "ymin = np.min(Y,axis=0)\n",
        "ymax = np.max(Y, axis =0)\n",
        "Y =(Y-ymin)/(ymax-ymin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5WT_9MKfTJ7",
        "outputId": "9a0c3569-a457-4139-b448-82d0a1e36c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.49065894 0.59474316 0.67672379 ... 0.38755433 0.7564279  0.61715662]\n",
            " [0.14930869 0.14660584 0.21278576 ... 0.33013413 0.8611836  0.78054803]\n",
            " [0.75726532 0.28513058 0.53845534 ... 0.34073049 0.84324579 0.7183105 ]\n",
            " ...\n",
            " [0.35184333 0.35741829 0.32881689 ... 0.28144271 0.77797859 0.75126493]\n",
            " [0.59522358 0.26502337 0.24656604 ... 0.45655311 0.78745769 0.81121808]\n",
            " [0.44168603 0.38385703 0.23216059 ... 0.73794624 0.78042383 0.88687287]]\n",
            "(2388, 60)\n",
            "[[0.60682784 0.36613679 0.40018899 ... 0.56614308 0.7649098  0.74976727]\n",
            " [0.20146498 0.28477631 0.23567695 ... 0.55204123 0.79510653 0.84210256]\n",
            " [0.54231908 0.52452663 0.68706399 ... 0.55400906 0.84151621 0.71154811]\n",
            " ...\n",
            " [0.14312142 0.18618695 0.10859767 ... 0.58530094 0.73401969 0.92233658]\n",
            " [0.56460525 0.40379632 0.3656651  ... 0.48405885 0.87487326 0.59238398]\n",
            " [0.61079894 0.60266386 0.59672189 ... 0.78073031 0.81549138 0.61062291]]\n",
            "(341, 60)\n",
            "[[0.21399436 0.27514885 0.31243362 ... 0.56827276 0.82542211 0.61645512]\n",
            " [0.09091961 0.1107108  0.14869354 ... 0.49907579 0.8397173  0.59704709]\n",
            " [0.2863094  0.35245959 0.33863872 ... 0.61793281 0.70084076 0.85021154]\n",
            " ...\n",
            " [0.07275674 0.07925112 0.12857018 ... 0.4356409  0.75425467 0.77463616]\n",
            " [0.60099494 0.39965245 0.33232475 ... 0.47335104 0.7789829  0.52355188]\n",
            " [0.06361192 0.11325333 0.01811514 ... 0.70138639 0.91620344 0.58147932]]\n",
            "(683, 60)\n"
          ]
        }
      ],
      "source": [
        "Xtrain = X[:math.floor(0.7*m),:]\n",
        "Xvalid = X[math.floor(0.7*m):math.floor(0.8*m),:]\n",
        "Xtest = X[math.floor(0.8*m):m,:]\n",
        "print(Xtrain)\n",
        "print(Xtrain.shape)\n",
        "print(Xvalid)\n",
        "print(Xvalid.shape)\n",
        "print(Xtest)\n",
        "print(Xtest.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z0s-l5Gj_Rq",
        "outputId": "fa81c15d-23a9-437f-9088-58c784247fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.         1.         1.         ... 1.         0.33333333 0.33333333]\n",
            "(2388,)\n",
            "[1.         1.         0.66666667 0.66666667 0.66666667 0.\n",
            " 0.33333333 0.         0.66666667 0.33333333 1.         0.33333333\n",
            " 0.66666667 1.         1.         0.66666667 0.         0.\n",
            " 0.33333333 0.         1.         0.         0.         1.\n",
            " 0.33333333 0.33333333 0.         1.         1.         1.\n",
            " 1.         0.66666667 0.         0.         0.33333333 0.66666667\n",
            " 0.33333333 0.         1.         0.33333333 0.66666667 1.\n",
            " 0.66666667 0.66666667 0.66666667 0.66666667 0.         0.\n",
            " 0.         0.33333333 0.         0.66666667 0.33333333 0.66666667\n",
            " 1.         0.33333333 0.         1.         1.         1.\n",
            " 0.66666667 0.         1.         0.         1.         0.33333333\n",
            " 0.33333333 1.         0.33333333 0.66666667 0.66666667 0.66666667\n",
            " 1.         0.         0.         0.         1.         0.66666667\n",
            " 0.33333333 1.         0.66666667 0.33333333 0.66666667 0.\n",
            " 0.33333333 0.33333333 1.         1.         0.66666667 0.66666667\n",
            " 0.66666667 0.66666667 0.66666667 0.         1.         1.\n",
            " 1.         0.66666667 0.33333333 1.         0.66666667 0.\n",
            " 0.         0.         1.         1.         0.66666667 0.66666667\n",
            " 0.         1.         0.66666667 0.33333333 0.33333333 0.33333333\n",
            " 1.         1.         0.         0.66666667 0.66666667 0.33333333\n",
            " 1.         0.33333333 1.         0.66666667 0.         1.\n",
            " 0.33333333 1.         0.33333333 1.         0.33333333 0.66666667\n",
            " 1.         0.         1.         0.33333333 0.66666667 0.\n",
            " 0.         0.66666667 0.66666667 0.66666667 1.         0.\n",
            " 0.33333333 1.         0.         1.         1.         0.66666667\n",
            " 1.         0.66666667 0.         0.66666667 1.         1.\n",
            " 0.         1.         0.66666667 1.         0.66666667 0.\n",
            " 0.66666667 0.66666667 0.         0.         1.         0.33333333\n",
            " 0.66666667 0.66666667 0.66666667 0.         0.66666667 1.\n",
            " 0.66666667 1.         0.66666667 1.         0.         0.\n",
            " 0.66666667 0.33333333 0.         0.66666667 0.33333333 0.33333333\n",
            " 0.         1.         1.         0.33333333 1.         1.\n",
            " 1.         0.33333333 0.66666667 0.         1.         1.\n",
            " 1.         0.66666667 0.66666667 1.         0.         1.\n",
            " 0.66666667 0.         0.         1.         0.66666667 0.33333333\n",
            " 0.33333333 1.         0.66666667 0.         0.66666667 0.66666667\n",
            " 0.66666667 0.66666667 0.         0.33333333 0.66666667 1.\n",
            " 0.33333333 1.         0.66666667 0.         0.         1.\n",
            " 0.66666667 0.66666667 0.         0.33333333 1.         0.66666667\n",
            " 0.         0.66666667 0.         0.         0.33333333 1.\n",
            " 0.33333333 0.         1.         0.33333333 0.33333333 0.\n",
            " 0.         0.33333333 0.         0.66666667 0.66666667 1.\n",
            " 0.66666667 0.         0.         0.66666667 0.33333333 1.\n",
            " 0.         0.66666667 0.33333333 0.66666667 0.66666667 0.33333333\n",
            " 0.66666667 0.66666667 0.66666667 0.66666667 0.         1.\n",
            " 1.         0.66666667 0.66666667 0.33333333 0.         0.33333333\n",
            " 0.33333333 0.33333333 1.         1.         0.         0.66666667\n",
            " 0.66666667 0.66666667 1.         0.33333333 0.33333333 1.\n",
            " 0.         0.66666667 0.66666667 1.         0.         0.33333333\n",
            " 0.         0.33333333 0.         0.         1.         0.33333333\n",
            " 0.33333333 1.         0.         0.33333333 0.66666667 0.66666667\n",
            " 1.         0.         0.33333333 0.33333333 0.66666667 0.66666667\n",
            " 0.         0.         0.33333333 1.         0.66666667 0.66666667\n",
            " 0.66666667 0.33333333 0.33333333 0.         0.66666667 0.33333333\n",
            " 1.         0.66666667 1.         1.         0.33333333 0.\n",
            " 1.         0.         1.         0.33333333 0.         0.\n",
            " 1.         0.33333333 0.         1.         0.        ]\n",
            "(341,)\n",
            "[0.         1.         0.         0.         0.66666667 0.\n",
            " 1.         0.33333333 1.         0.         0.         0.\n",
            " 0.         0.         0.         0.66666667 0.66666667 0.33333333\n",
            " 0.33333333 0.         0.33333333 0.33333333 0.         0.66666667\n",
            " 0.         0.33333333 0.         0.66666667 0.33333333 0.\n",
            " 0.33333333 1.         0.66666667 0.66666667 0.33333333 0.33333333\n",
            " 1.         1.         0.33333333 1.         0.66666667 0.33333333\n",
            " 0.66666667 0.         0.33333333 0.66666667 0.66666667 1.\n",
            " 0.         1.         0.66666667 0.         0.         1.\n",
            " 0.33333333 0.33333333 0.33333333 0.         0.         1.\n",
            " 0.         0.33333333 0.66666667 1.         0.66666667 1.\n",
            " 1.         0.         0.         0.33333333 1.         0.\n",
            " 1.         0.66666667 0.33333333 0.33333333 0.33333333 0.\n",
            " 1.         0.         0.         0.         0.33333333 0.66666667\n",
            " 0.66666667 0.         0.33333333 0.         0.         0.33333333\n",
            " 0.         0.33333333 0.66666667 1.         0.33333333 1.\n",
            " 0.33333333 1.         0.         0.66666667 0.         0.66666667\n",
            " 0.66666667 0.33333333 0.66666667 1.         0.         0.33333333\n",
            " 0.33333333 0.33333333 0.66666667 1.         0.         0.\n",
            " 0.66666667 0.33333333 0.33333333 0.         1.         0.\n",
            " 0.33333333 0.33333333 1.         0.         0.66666667 0.33333333\n",
            " 0.         0.66666667 0.66666667 1.         0.66666667 0.\n",
            " 0.         0.66666667 1.         1.         1.         0.\n",
            " 0.         1.         1.         0.66666667 0.33333333 0.33333333\n",
            " 0.         0.         0.33333333 0.66666667 0.33333333 0.66666667\n",
            " 0.66666667 0.66666667 0.33333333 0.66666667 0.33333333 1.\n",
            " 0.66666667 0.66666667 0.66666667 1.         0.33333333 0.\n",
            " 1.         1.         1.         0.33333333 0.33333333 1.\n",
            " 0.33333333 0.66666667 0.         0.66666667 0.         0.\n",
            " 1.         0.33333333 1.         0.         0.33333333 0.\n",
            " 0.66666667 0.         0.66666667 0.         0.         0.33333333\n",
            " 1.         0.66666667 0.33333333 0.66666667 0.33333333 0.66666667\n",
            " 0.33333333 0.66666667 0.66666667 0.33333333 1.         0.\n",
            " 0.33333333 0.66666667 0.         0.33333333 0.66666667 0.33333333\n",
            " 0.33333333 0.         0.         0.66666667 0.         0.33333333\n",
            " 0.         0.33333333 0.         0.66666667 0.         1.\n",
            " 1.         0.33333333 1.         0.66666667 1.         0.\n",
            " 0.66666667 0.33333333 1.         1.         1.         0.\n",
            " 1.         0.66666667 0.         0.33333333 0.66666667 0.66666667\n",
            " 0.33333333 1.         1.         0.         1.         1.\n",
            " 0.         0.33333333 0.33333333 0.66666667 0.33333333 0.66666667\n",
            " 0.66666667 0.         1.         0.66666667 0.33333333 0.66666667\n",
            " 0.33333333 0.         0.66666667 1.         0.66666667 1.\n",
            " 0.33333333 0.33333333 0.         0.33333333 0.33333333 0.66666667\n",
            " 0.66666667 1.         0.33333333 0.         1.         0.33333333\n",
            " 1.         1.         0.33333333 0.66666667 0.         0.33333333\n",
            " 0.         0.33333333 0.         1.         0.33333333 0.66666667\n",
            " 0.         0.33333333 1.         0.         0.66666667 0.33333333\n",
            " 0.33333333 0.66666667 1.         0.33333333 1.         0.\n",
            " 0.         0.         1.         0.33333333 0.33333333 0.33333333\n",
            " 0.         0.         0.         0.33333333 0.         1.\n",
            " 1.         1.         1.         0.33333333 0.33333333 1.\n",
            " 0.66666667 0.         0.66666667 0.         0.33333333 1.\n",
            " 0.66666667 0.33333333 0.33333333 1.         0.33333333 0.33333333\n",
            " 0.33333333 1.         0.         0.         0.66666667 0.33333333\n",
            " 0.66666667 1.         1.         0.         0.66666667 0.66666667\n",
            " 0.66666667 1.         0.33333333 1.         0.66666667 0.\n",
            " 0.         0.33333333 0.66666667 0.33333333 0.         0.33333333\n",
            " 1.         1.         0.66666667 0.33333333 1.         0.33333333\n",
            " 0.         0.33333333 0.33333333 0.66666667 1.         0.\n",
            " 0.66666667 0.         0.33333333 0.         0.66666667 0.33333333\n",
            " 0.66666667 1.         0.         0.         0.33333333 0.\n",
            " 1.         0.66666667 0.         0.33333333 1.         0.33333333\n",
            " 0.66666667 0.66666667 0.66666667 0.         0.33333333 0.33333333\n",
            " 0.66666667 0.33333333 0.         0.66666667 0.33333333 0.66666667\n",
            " 0.66666667 0.         0.66666667 1.         0.66666667 0.33333333\n",
            " 1.         1.         1.         0.33333333 0.66666667 0.33333333\n",
            " 0.33333333 0.         0.         1.         0.66666667 0.66666667\n",
            " 1.         0.66666667 0.         1.         0.         0.66666667\n",
            " 0.         1.         0.33333333 1.         0.33333333 1.\n",
            " 1.         0.         1.         0.33333333 0.33333333 1.\n",
            " 0.         0.33333333 0.66666667 0.33333333 0.66666667 0.33333333\n",
            " 1.         1.         1.         1.         1.         0.33333333\n",
            " 0.66666667 0.         0.66666667 0.66666667 0.66666667 0.33333333\n",
            " 1.         0.         0.         0.         1.         0.66666667\n",
            " 1.         0.33333333 0.66666667 0.         1.         0.\n",
            " 0.66666667 0.66666667 0.         0.33333333 0.66666667 0.66666667\n",
            " 0.         0.66666667 0.         0.33333333 0.66666667 1.\n",
            " 0.33333333 0.33333333 0.66666667 1.         0.33333333 0.33333333\n",
            " 0.33333333 1.         0.66666667 0.33333333 0.33333333 0.\n",
            " 0.33333333 0.66666667 0.33333333 0.33333333 1.         0.\n",
            " 0.33333333 1.         1.         0.33333333 0.66666667 0.\n",
            " 1.         0.66666667 0.         0.33333333 0.66666667 0.33333333\n",
            " 0.         1.         0.33333333 0.66666667 0.66666667 0.66666667\n",
            " 0.         0.         0.         0.33333333 0.66666667 0.66666667\n",
            " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.\n",
            " 1.         0.33333333 0.33333333 0.66666667 0.33333333 0.\n",
            " 1.         0.66666667 0.33333333 1.         0.         1.\n",
            " 1.         1.         1.         1.         1.         0.33333333\n",
            " 1.         0.         1.         1.         0.66666667 1.\n",
            " 0.66666667 0.66666667 1.         0.33333333 0.66666667 0.66666667\n",
            " 0.         0.66666667 0.66666667 0.33333333 0.         0.66666667\n",
            " 0.66666667 0.33333333 0.66666667 0.         1.         0.66666667\n",
            " 0.         0.33333333 0.33333333 0.66666667 0.         0.66666667\n",
            " 0.33333333 0.66666667 0.         1.         1.         0.33333333\n",
            " 0.33333333 0.66666667 0.33333333 0.33333333 0.         0.66666667\n",
            " 0.         1.         1.         1.         1.         1.\n",
            " 0.33333333 0.33333333 0.66666667 0.33333333 1.         0.33333333\n",
            " 0.33333333 0.66666667 1.         0.33333333 1.         0.\n",
            " 0.         0.66666667 0.         1.         1.         0.33333333\n",
            " 1.         0.66666667 0.66666667 0.         1.         0.\n",
            " 1.         1.         0.66666667 0.66666667 0.33333333 0.\n",
            " 0.33333333 1.         0.33333333 1.         0.33333333 0.\n",
            " 0.66666667 0.         0.         0.66666667 1.         0.66666667\n",
            " 1.         0.66666667 0.         0.66666667 0.33333333 1.\n",
            " 0.66666667 1.         0.         0.66666667 0.         1.\n",
            " 0.         0.         1.         1.         1.         0.\n",
            " 1.         1.         0.         0.         1.         1.\n",
            " 0.66666667 0.         1.         0.66666667 0.66666667 0.66666667\n",
            " 0.         0.33333333 0.66666667 0.66666667 1.         0.66666667\n",
            " 1.         0.         0.33333333 1.         0.66666667 0.66666667\n",
            " 1.         0.         0.66666667 1.         1.         1.\n",
            " 0.66666667 0.66666667 0.         0.33333333 0.66666667 0.\n",
            " 0.         1.         0.33333333 1.         1.        ]\n",
            "(683,)\n"
          ]
        }
      ],
      "source": [
        "Ytrain = Y[0:math.floor(0.7*m)]\n",
        "Yvalid = Y[math.floor(0.7*m):math.floor(0.8*m)]\n",
        "Ytest = Y[math.floor(0.8*m):m]\n",
        "print(Ytrain)\n",
        "print(Ytrain.shape)\n",
        "print(Yvalid)\n",
        "print(Yvalid.shape)\n",
        "print(Ytest)\n",
        "print(Ytest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMlwsz8Dos41"
      },
      "source": [
        "Now, we shall try and implement one vs one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odVxZSDZqjho",
        "outputId": "38db6821-4262-4305-e714-b34b07487ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "(1, 60)\n"
          ]
        }
      ],
      "source": [
        "w = np.zeros((1,Xtrain.shape[1]))\n",
        "print(w)\n",
        "print(w.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcy07JYTuMsT"
      },
      "source": [
        "As there are 4 classes we shall make 4 models and obtain values. This is also known as One vs All Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lujYV3pezJp8"
      },
      "source": [
        "Model 1 where everything other than 0 is 0 and 0 is 1. For model 2, everything other than 0.33 is 0. For model 2, everything other than 0.66 is 0. And for model 4 everything other than 1 is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2js3raXzT_k"
      },
      "outputs": [],
      "source": [
        "Ytrain_M1 = np.zeros(Ytrain.shape[0])\n",
        "Ytrain_M2 = np.zeros(Ytrain.shape[0])\n",
        "Ytrain_M3 = np.zeros(Ytrain.shape[0])\n",
        "Ytrain_M4 = np.zeros(Ytrain.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qZqQVJYippb",
        "outputId": "3045bce3-36da-4d62-d988-ff036b019ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "for i in range (0,Ytrain.shape[0]):\n",
        "  if(Ytrain[i]==0):\n",
        "    Ytrain_M1[i]=1\n",
        "  else:\n",
        "    Ytrain_M1[i]=0\n",
        "\n",
        "print(Ytrain_M1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lepeITuJuU4w",
        "outputId": "0cf27585-e19d-42cf-b2a4-46dda69f829e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. ... 0. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "for i in range (0,Ytrain.shape[0]):\n",
        "  if(round(Ytrain[i],2)==0.33):\n",
        "    Ytrain_M2[i]=1\n",
        "  else:\n",
        "    Ytrain_M2[i]=0\n",
        "\n",
        "print(Ytrain_M2)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jvngybyiY0s",
        "outputId": "74a072a5-d406-481f-8b9d-218c7cbbfa00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "for i in range (0,Ytrain.shape[0]):\n",
        "  if(round(Ytrain[i],2)==0.67):\n",
        "    Ytrain_M3[i]=1\n",
        "  else:\n",
        "    Ytrain_M3[i]=0\n",
        "\n",
        "print(Ytrain_M3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMMvZaZ2idmM",
        "outputId": "6b3a1a66-6972-4aad-ce52-45ef9bda8627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. ... 1. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "for i in range (0,Ytrain.shape[0]):\n",
        "  if(Ytrain[i]==1):\n",
        "    Ytrain_M4[i]=1\n",
        "  else:\n",
        "    Ytrain_M4[i]=0\n",
        "\n",
        "print(Ytrain_M4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZu1YWK-i9UO"
      },
      "outputs": [],
      "source": [
        "alphaoptions = np.array([0.15,0.1,0.18])\n",
        "lamboptions = np.array([0.3,0.35,0.4])\n",
        "iters = 12000 # read code ahead to find why iterations is no longer a grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0fkPovI788I",
        "outputId": "0225511d-8bf4-46d2-ef42-3e3dd0de31a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4. 4. 3. 3. 3. 1. 2. 1. 3. 2. 4. 2. 3. 4. 4. 3. 1. 1. 2. 1. 4. 1. 1. 4.\n",
            " 2. 2. 1. 4. 4. 4. 4. 3. 1. 1. 2. 3. 2. 1. 4. 2. 3. 4. 3. 3. 3. 3. 1. 1.\n",
            " 1. 2. 1. 3. 2. 3. 4. 2. 1. 4. 4. 4. 3. 1. 4. 1. 4. 2. 2. 4. 2. 3. 3. 3.\n",
            " 4. 1. 1. 1. 4. 3. 2. 4. 3. 2. 3. 1. 2. 2. 4. 4. 3. 3. 3. 3. 3. 1. 4. 4.\n",
            " 4. 3. 2. 4. 3. 1. 1. 1. 4. 4. 3. 3. 1. 4. 3. 2. 2. 2. 4. 4. 1. 3. 3. 2.\n",
            " 4. 2. 4. 3. 1. 4. 2. 4. 2. 4. 2. 3. 4. 1. 4. 2. 3. 1. 1. 3. 3. 3. 4. 1.\n",
            " 2. 4. 1. 4. 4. 3. 4. 3. 1. 3. 4. 4. 1. 4. 3. 4. 3. 1. 3. 3. 1. 1. 4. 2.\n",
            " 3. 3. 3. 1. 3. 4. 3. 4. 3. 4. 1. 1. 3. 2. 1. 3. 2. 2. 1. 4. 4. 2. 4. 4.\n",
            " 4. 2. 3. 1. 4. 4. 4. 3. 3. 4. 1. 4. 3. 1. 1. 4. 3. 2. 2. 4. 3. 1. 3. 3.\n",
            " 3. 3. 1. 2. 3. 4. 2. 4. 3. 1. 1. 4. 3. 3. 1. 2. 4. 3. 1. 3. 1. 1. 2. 4.\n",
            " 2. 1. 4. 2. 2. 1. 1. 2. 1. 3. 3. 4. 3. 1. 1. 3. 2. 4. 1. 3. 2. 3. 3. 2.\n",
            " 3. 3. 3. 3. 1. 4. 4. 3. 3. 2. 1. 2. 2. 2. 4. 4. 1. 3. 3. 3. 4. 2. 2. 4.\n",
            " 1. 3. 3. 4. 1. 2. 1. 2. 1. 1. 4. 2. 2. 4. 1. 2. 3. 3. 4. 1. 2. 2. 3. 3.\n",
            " 1. 1. 2. 4. 3. 3. 3. 2. 2. 1. 3. 2. 4. 3. 4. 4. 2. 1. 4. 1. 4. 2. 1. 1.\n",
            " 4. 2. 1. 4. 1.]\n"
          ]
        }
      ],
      "source": [
        "for i in range (0,341):\n",
        "  if Yvalid[i] == 0:\n",
        "    Yvalid[i] = 1\n",
        "  elif round(Yvalid[i],2) == 0.33:\n",
        "    Yvalid[i] = 2\n",
        "  elif round(Yvalid[i],2) == 0.67:\n",
        "    Yvalid[i] = 3\n",
        "  else:\n",
        "    Yvalid[i] = 4\n",
        "\n",
        "\n",
        "print(Yvalid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NFHJS-HyRbU_",
        "outputId": "349a4341-24f5-4d50-9f5d-e5b9ff0268b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for LOR_BGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.28152492668621704 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.2903225806451613 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.2903225806451613 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.23460410557184752 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.2697947214076246 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.2697947214076246 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.20234604105571846 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.15 , lambda =  0.3  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_BGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.28152492668621704 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.26099706744868034 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.2668621700879765 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.22287390029325513 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.20234604105571846 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.20234604105571846 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.24926686217008798 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.20234604105571846 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.15 , lambda =  0.35  and iterations =  12000  is  0.2844574780058651 \n",
            "\n",
            "Accuracy for LOR_BGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.28152492668621704 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.2961876832844575 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.2375366568914956 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.22580645161290322 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.18475073313782991 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.15 , lambda =  0.4  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_BGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.2727272727272727 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.25219941348973607 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.30498533724340177 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.23460410557184752 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.24633431085043989 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.25806451612903225 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_BGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.2727272727272727 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.1935483870967742 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.2697947214076246 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.22287390029325513 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.2697947214076246 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.26099706744868034 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.1 , lambda =  0.35  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_BGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.2727272727272727 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.23460410557184752 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.22580645161290322 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.22580645161290322 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.2434017595307918 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.24633431085043989 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.1 , lambda =  0.4  and iterations =  12000  is  0.2697947214076246 \n",
            "\n",
            "Accuracy for LOR_BGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.2668621700879765 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.25219941348973607 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.23460410557184752 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.18 , lambda =  0.3  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_BGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.2668621700879765 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.25513196480938416 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.2668621700879765 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.22287390029325513 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.17302052785923755 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.20234604105571846 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.18 , lambda =  0.35  and iterations =  12000  is  0.20234604105571846 \n",
            "\n",
            "Accuracy for LOR_BGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.2668621700879765 \n",
            "\n",
            "Accuracy for LOR_MBGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.26392961876832843 \n",
            "\n",
            "Accuracy for LOR_SGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.2434017595307918 \n",
            "\n",
            "Accuracy for LOR_L2_BGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.22580645161290322 \n",
            "\n",
            "Accuracy for LOR_L2_MBGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L2_SGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.2873900293255132 \n",
            "\n",
            "Accuracy for LOR_L1_BGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.2404692082111437 \n",
            "\n",
            "Accuracy for LOR_L1_MBGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.20234604105571846 \n",
            "\n",
            "Accuracy for LOR_L1_SGD at alpha =  0.18 , lambda =  0.4  and iterations =  12000  is  0.21114369501466276 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "# After having previously run a grid search we obtained the best value at \"Accuracy for LOR_SGD at alpha =  0.1 , lambda =  0.3  and iterations =  12000  is  0.2961876832844575\" so we have altered our grid search by a bit and removed number of iterations and set it fixed to 12000\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "n_epochs = 5000\n",
        "for i in range(0,3):\n",
        "  alpha = alphaoptions[i]\n",
        "  for k in range(0,3):\n",
        "      lamb = lamboptions[k] \n",
        "      # LOR WITH BGD\n",
        "      Ypred_LOR_BGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_BGD_ModelValue = LOR_BGD(alpha,iters)\n",
        "      Ypred_LOR_BGD = np.append(Ypred_LOR_BGD,Ypred_LOR_BGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_BGD_ModelValue = LOR_BGD(alpha,iters)\n",
        "      Ypred_LOR_BGD = np.append(Ypred_LOR_BGD,Ypred_LOR_BGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_BGD_ModelValue = LOR_BGD(alpha,iters)\n",
        "      Ypred_LOR_BGD = np.append(Ypred_LOR_BGD,Ypred_LOR_BGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_BGD_ModelValue = LOR_BGD(alpha,iters)\n",
        "      Ypred_LOR_BGD = np.append(Ypred_LOR_BGD,Ypred_LOR_BGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_BGD = np.delete(Ypred_LOR_BGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_BGD = np.argmax(Ypred_LOR_BGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_BGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_BGD = np.asmatrix(Ypred_LOR_BGD)\n",
        "      print(\"Accuracy for LOR_BGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_BGD),\"\\n\") # we find accuracy at particular value\n",
        "      \n",
        "      \n",
        "      # LOR WITH MBGD\n",
        "      Ypred_LOR_MBGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_MBGD_ModelValue = LOR_MBGD(alpha,iters,batch_size)\n",
        "      Ypred_LOR_MBGD = np.append(Ypred_LOR_MBGD,Ypred_LOR_MBGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_MBGD_ModelValue = LOR_MBGD(alpha,iters,batch_size)\n",
        "      Ypred_LOR_MBGD = np.append(Ypred_LOR_MBGD,Ypred_LOR_MBGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_MBGD_ModelValue = LOR_MBGD(alpha,iters,batch_size)\n",
        "      Ypred_LOR_MBGD = np.append(Ypred_LOR_MBGD,Ypred_LOR_MBGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_MBGD_ModelValue = LOR_MBGD(alpha,iters,batch_size)\n",
        "      Ypred_LOR_MBGD = np.append(Ypred_LOR_MBGD,Ypred_LOR_MBGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_MBGD = np.delete(Ypred_LOR_MBGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_MBGD = np.argmax(Ypred_LOR_MBGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_MBGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_MBGD = np.asmatrix(Ypred_LOR_MBGD)\n",
        "      print(\"Accuracy for LOR_MBGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_MBGD),\"\\n\") # we find accuracy at particular value\n",
        "\n",
        "      # LOR with SGD\n",
        "      Ypred_LOR_SGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_SGD_ModelValue = LOR_SGD(alpha,iters)\n",
        "      Ypred_LOR_SGD = np.append(Ypred_LOR_SGD,Ypred_LOR_SGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_SGD_ModelValue = LOR_SGD(alpha,iters)\n",
        "      Ypred_LOR_SGD = np.append(Ypred_LOR_SGD,Ypred_LOR_SGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_SGD_ModelValue = LOR_SGD(alpha,iters)\n",
        "      Ypred_LOR_SGD = np.append(Ypred_LOR_SGD,Ypred_LOR_SGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_SGD_ModelValue = LOR_SGD(alpha,iters)\n",
        "      Ypred_LOR_SGD = np.append(Ypred_LOR_SGD,Ypred_LOR_SGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_SGD = np.delete(Ypred_LOR_SGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_SGD = np.argmax(Ypred_LOR_SGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_SGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_SGD = np.asmatrix(Ypred_LOR_SGD)\n",
        "      print(\"Accuracy for LOR_SGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_SGD),\"\\n\") # we find accuracy at particular value\n",
        "      \n",
        "      \n",
        "      # LOR with L2 and BGD\n",
        "      Ypred_LOR_L2_BGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_L2_BGD_ModelValue = LOR_L2_BGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L2_BGD = np.append(Ypred_LOR_L2_BGD,Ypred_LOR_L2_BGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_L2_BGD_ModelValue = LOR_L2_BGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L2_BGD = np.append(Ypred_LOR_L2_BGD,Ypred_LOR_L2_BGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_L2_BGD_ModelValue = LOR_L2_BGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L2_BGD = np.append(Ypred_LOR_L2_BGD,Ypred_LOR_L2_BGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_L2_BGD_ModelValue = LOR_L2_BGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L2_BGD = np.append(Ypred_LOR_L2_BGD,Ypred_LOR_L2_BGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_L2_BGD = np.delete(Ypred_LOR_L2_BGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_L2_BGD = np.argmax(Ypred_LOR_L2_BGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_L2_BGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_L2_BGD = np.asmatrix(Ypred_LOR_L2_BGD)\n",
        "      print(\"Accuracy for LOR_L2_BGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_L2_BGD),\"\\n\") # we find accuracy at particular value\n",
        "      \n",
        "      \n",
        "      # LOR with L2 and MBGD\n",
        "      Ypred_LOR_L2_MBGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_L2_MBGD_ModelValue = LOR_L2_MBGD(alpha,iters,batch_size,lamb)\n",
        "      Ypred_LOR_L2_MBGD = np.append(Ypred_LOR_L2_MBGD,Ypred_LOR_L2_MBGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_L2_MBGD_ModelValue = LOR_L2_MBGD(alpha,iters,batch_size,lamb)\n",
        "      Ypred_LOR_L2_MBGD = np.append(Ypred_LOR_L2_MBGD,Ypred_LOR_L2_MBGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_L2_MBGD_ModelValue = LOR_L2_MBGD(alpha,iters,batch_size,lamb)\n",
        "      Ypred_LOR_L2_MBGD = np.append(Ypred_LOR_L2_MBGD,Ypred_LOR_L2_MBGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_L2_MBGD_ModelValue = LOR_L2_MBGD(alpha,iters,batch_size,lamb)\n",
        "      Ypred_LOR_L2_MBGD = np.append(Ypred_LOR_L2_MBGD,Ypred_LOR_L2_MBGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_L2_MBGD = np.delete(Ypred_LOR_L2_MBGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_L2_MBGD = np.argmax(Ypred_LOR_L2_MBGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_L2_MBGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_L2_MBGD = np.asmatrix(Ypred_LOR_L2_MBGD)\n",
        "      print(\"Accuracy for LOR_L2_MBGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_L2_MBGD),\"\\n\") # we find accuracy at particular value\n",
        "      \n",
        "      \n",
        "      # LOR with L2 and SGD\n",
        "      Ypred_LOR_L2_SGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_L2_SGD_ModelValue = LOR_L2_SGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L2_SGD = np.append(Ypred_LOR_L2_SGD,Ypred_LOR_L2_SGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_L2_SGD_ModelValue = LOR_L2_SGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L2_SGD = np.append(Ypred_LOR_L2_SGD,Ypred_LOR_L2_SGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_L2_SGD_ModelValue = LOR_L2_SGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L2_SGD = np.append(Ypred_LOR_L2_SGD,Ypred_LOR_L2_SGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_L2_SGD_ModelValue = LOR_L2_SGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L2_SGD = np.append(Ypred_LOR_L2_SGD,Ypred_LOR_L2_SGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_L2_SGD = np.delete(Ypred_LOR_L2_SGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_L2_SGD = np.argmax(Ypred_LOR_L2_SGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_L2_SGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_L2_SGD = np.asmatrix(Ypred_LOR_L2_SGD)\n",
        "      print(\"Accuracy for LOR_L2_SGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_L2_SGD),\"\\n\") # we find accuracy at particular value\n",
        "\n",
        "      # LOR with L1 and BGD\n",
        "      Ypred_LOR_L1_BGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_L1_BGD_ModelValue = LOR_L1_BGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L1_BGD = np.append(Ypred_LOR_L1_BGD,Ypred_LOR_L1_BGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_L1_BGD_ModelValue = LOR_L1_BGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L1_BGD = np.append(Ypred_LOR_L1_BGD,Ypred_LOR_L1_BGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_L1_BGD_ModelValue = LOR_L1_BGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L1_BGD = np.append(Ypred_LOR_L1_BGD,Ypred_LOR_L1_BGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_L1_BGD_ModelValue = LOR_L1_BGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L1_BGD = np.append(Ypred_LOR_L1_BGD,Ypred_LOR_L1_BGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_L1_BGD = np.delete(Ypred_LOR_L1_BGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_L1_BGD = np.argmax(Ypred_LOR_L1_BGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_L1_BGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_L1_BGD = np.asmatrix(Ypred_LOR_L1_BGD)\n",
        "      print(\"Accuracy for LOR_L1_BGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_L1_BGD),\"\\n\") # we find accuracy at particular value\n",
        "\n",
        "      # LOR with L1 and MBGD\n",
        "      Ypred_LOR_L1_MBGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_L1_MBGD_ModelValue = LOR_L1_MBGD(alpha,iters,batch_size,lamb)\n",
        "      Ypred_LOR_L1_MBGD = np.append(Ypred_LOR_L1_MBGD,Ypred_LOR_L1_MBGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_L1_MBGD_ModelValue = LOR_L1_MBGD(alpha,iters,batch_size,lamb)\n",
        "      Ypred_LOR_L1_MBGD = np.append(Ypred_LOR_L1_MBGD,Ypred_LOR_L1_MBGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_L1_MBGD_ModelValue = LOR_L1_MBGD(alpha,iters,batch_size,lamb)\n",
        "      Ypred_LOR_L1_MBGD = np.append(Ypred_LOR_L1_MBGD,Ypred_LOR_L1_MBGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_L1_MBGD_ModelValue = LOR_L1_MBGD(alpha,iters,batch_size,lamb)\n",
        "      Ypred_LOR_L1_MBGD = np.append(Ypred_LOR_L1_MBGD,Ypred_LOR_L1_MBGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_L1_MBGD = np.delete(Ypred_LOR_L1_MBGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_L1_MBGD = np.argmax(Ypred_LOR_L1_MBGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_L1_MBGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_L1_MBGD = np.asmatrix(Ypred_LOR_L1_MBGD)\n",
        "      print(\"Accuracy for LOR_L1_MBGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_L1_MBGD),\"\\n\") # we find accuracy at particular value\n",
        "\n",
        "      # LOR with L1 and SGD\n",
        "      Ypred_LOR_L1_SGD = np.ones([341,1]) #we make a dummy arrray\n",
        "      Ytr = np.asmatrix(Ytrain_M1).T\n",
        "      Ypred_LOR_L1_SGD_ModelValue = LOR_L1_SGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L1_SGD = np.append(Ypred_LOR_L1_SGD,Ypred_LOR_L1_SGD_ModelValue,axis=1) #we append M1 values\n",
        "      Ytr = np.asmatrix(Ytrain_M2).T\n",
        "      Ypred_LOR_L1_SGD_ModelValue = LOR_L1_SGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L1_SGD = np.append(Ypred_LOR_L1_SGD,Ypred_LOR_L1_SGD_ModelValue,axis=1) #we append M2 values\n",
        "      Ytr = np.asmatrix(Ytrain_M3).T\n",
        "      Ypred_LOR_L1_SGD_ModelValue = LOR_L1_SGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L1_SGD = np.append(Ypred_LOR_L1_SGD,Ypred_LOR_L1_SGD_ModelValue,axis=1) #we append M3 values\n",
        "      Ytr = np.asmatrix(Ytrain_M4).T\n",
        "      Ypred_LOR_L1_SGD_ModelValue = LOR_L1_SGD(alpha,iters,lamb)\n",
        "      Ypred_LOR_L1_SGD = np.append(Ypred_LOR_L1_SGD,Ypred_LOR_L1_SGD_ModelValue,axis=1) #we append M4 values\n",
        "      Ypred_LOR_L1_SGD = np.delete(Ypred_LOR_L1_SGD, 0, 1) #we delete dummy ones column\n",
        "      Ypred_LOR_L1_SGD = np.argmax(Ypred_LOR_L1_SGD, axis=1)+1 #we assign class labels based on probabilities\n",
        "      #print(Ypred_LOR_L1_SGD)\n",
        "      Yvalid = np.asmatrix(Yvalid)\n",
        "      Ypred_LOR_L1_SGD = np.asmatrix(Ypred_LOR_L1_SGD)\n",
        "      print(\"Accuracy for LOR_L1_SGD at alpha = \",alpha,\", lambda = \",lamb,\" and iterations = \",iters,\" is \",skm.accuracy_score(Yvalid.T, Ypred_LOR_L1_SGD),\"\\n\") # we find accuracy at particular value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have used our validation data to find our optimum parameter values, we will employ these values to our test data. Our optimum values are alpha = 0.1, lambda = 0.3 and iterations = 12000. Here we will only employ LOR with SGD case but other cases can be employed simillarly as shown above."
      ],
      "metadata": {
        "id": "4MSicdc0fcDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we shall implement the One vs One Model."
      ],
      "metadata": {
        "id": "7LNgP-YHVhpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_onevsone = pd.read_excel('/content/drive/MyDrive/data_Q4.xlsx',header=None)\n",
        "print(data_onevsone)\n",
        "print(data_onevsone.shape)"
      ],
      "metadata": {
        "id": "XSdyFEMLVy6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc7c98b-2a96-4ce1-e609-834ddb2bae4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              0           1           2           3           4           5   \\\n",
            "0     242.751526  281.801298  250.032405  132.099709   61.166502  247.837695   \n",
            "1     216.585951  297.057883  300.938478  131.358295  105.976730  273.299350   \n",
            "2     265.735536  339.271134  269.817305  102.304781   83.869539  281.962656   \n",
            "3     207.631953  255.284282  254.563071  229.883639   87.492384  303.314846   \n",
            "4     205.016124  333.265999  536.342842  106.237587  155.700409  272.692251   \n",
            "...          ...         ...         ...         ...         ...         ...   \n",
            "3407  723.913528  343.529660  360.468834  233.711682  177.309124  717.744261   \n",
            "3408  466.593370  215.858228  235.940729  309.475340  126.789443  333.123293   \n",
            "3409  446.227198  219.936910  181.605753  160.483773  106.179660  416.263221   \n",
            "3410  511.406437  215.379710  170.598957   89.543149   73.107090  457.783309   \n",
            "3411  757.967516  268.431243  189.755280  156.957408  126.298293  835.417941   \n",
            "\n",
            "              6           7           8           9   ...        51        52  \\\n",
            "0     306.999281  271.560155  126.038604   74.050379  ...  1.891884  1.612099   \n",
            "1     380.251154  335.864007  109.527577  113.088266  ...  1.779043  1.592470   \n",
            "2     386.806725  347.201912  105.945737  104.016513  ...  1.524681  1.763061   \n",
            "3     310.329644  276.926943  207.445146  108.620999  ...  1.907888  1.638839   \n",
            "4     367.533485  537.061350  110.167703  133.100524  ...  1.818213  1.860656   \n",
            "...          ...         ...         ...         ...  ...       ...       ...   \n",
            "3407  448.791348  379.113979  248.029948  170.735388  ...  1.928222  1.659960   \n",
            "3408  200.966375  173.674275  301.590225  110.481002  ...  1.841637  1.836036   \n",
            "3409  251.314023  196.936825  247.802048  114.649466  ...  1.953375  1.831422   \n",
            "3410  230.770136  178.846709  123.660090   78.208996  ...  1.897879  1.903539   \n",
            "3411  337.510327  247.097266  257.882395  180.132129  ...  1.906428  1.674688   \n",
            "\n",
            "            53        54        55        56        57        58        59  60  \n",
            "0     1.396853  1.437446  1.880098  1.872789  1.603211  1.281972  1.844860   1  \n",
            "1     1.749967  1.430117  1.935712  1.739076  1.572927  1.598582  1.625395   1  \n",
            "2     1.935089  1.642102  1.929802  1.522725  1.849940  1.848160  1.706134   1  \n",
            "3     1.604410  1.683284  1.845578  1.978957  1.682881  1.710404  1.664391   1  \n",
            "4     1.789285  1.606024  1.758654  1.834728  1.740251  1.703264  1.535684   1  \n",
            "...        ...       ...       ...       ...       ...       ...       ...  ..  \n",
            "3407  1.658291  1.640978  1.888330  1.933384  1.661284  1.948758  1.758541   4  \n",
            "3408  1.556465  1.898255  1.960397  1.973080  1.740866  1.568558  1.494537   4  \n",
            "3409  1.673341  1.889050  1.862514  2.004401  1.861006  1.418510  1.772295   4  \n",
            "3410  1.836154  1.841137  1.789531  1.837314  1.957492  1.609298  1.664890   4  \n",
            "3411  1.805059  1.529764  1.835415  2.056879  1.732298  1.790039  1.560633   4  \n",
            "\n",
            "[3412 rows x 61 columns]\n",
            "(3412, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_onevsone_values = data_onevsone.values\n",
        "print(data_onevsone_values)\n",
        "data_onevsone_values = np.take(data_onevsone_values,np.random.permutation(data_onevsone_values.shape[0]),axis=0,out=data_onevsone_values)\n",
        "print(data_onevsone_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsErSbcEa8h0",
        "outputId": "669226c9-5b59-408b-fddb-c1ac794e29b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[242.75152634 281.80129772 250.03240457 ...   1.28197209   1.8448603\n",
            "    1.        ]\n",
            " [216.58595112 297.05788313 300.9384782  ...   1.59858185   1.62539544\n",
            "    1.        ]\n",
            " [265.73553593 339.27113445 269.81730525 ...   1.84815977   1.70613365\n",
            "    1.        ]\n",
            " ...\n",
            " [446.22719796 219.93690991 181.60575345 ...   1.41851014   1.77229548\n",
            "    4.        ]\n",
            " [511.40643675 215.37971012 170.59895734 ...   1.60929772   1.66489041\n",
            "    4.        ]\n",
            " [757.96751558 268.43124317 189.75528024 ...   1.79003949   1.56063308\n",
            "    4.        ]]\n",
            "[[575.85427001 446.49012138 488.8665109  ...   1.66315922   1.54580234\n",
            "    2.        ]\n",
            " [238.77840725 122.07767083 196.98130991 ...   1.59301201   1.62768437\n",
            "    3.        ]\n",
            " [304.26438833 352.97378125 338.93499076 ...   1.79031258   1.56720704\n",
            "    1.        ]\n",
            " ...\n",
            " [454.24631909 938.95513227 337.74444131 ...   1.80235446   1.81591508\n",
            "    2.        ]\n",
            " [538.3450476  570.92391022 402.28356465 ...   1.57945617   1.37450674\n",
            "    2.        ]\n",
            " [434.3616972  372.26806379 371.8911643  ...   1.67991659   1.64858147\n",
            "    4.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_onevsone_values_train = data_onevsone_values[:math.floor(0.7*m),:]\n",
        "data_onevsone_values_valid = data_onevsone_values[math.floor(0.7*m):math.floor(0.8*m),:]\n",
        "data_onevsone_values_test = data_onevsone_values[math.floor(0.8*m):m,:]\n",
        "print(data_onevsone_values_train)\n",
        "print(data_onevsone_values_train.shape)\n",
        "print(data_onevsone_values_valid)\n",
        "print(data_onevsone_values_valid.shape)\n",
        "print(data_onevsone_values_test)\n",
        "print(data_onevsone_values_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfu2MeU3bNyt",
        "outputId": "895f732e-9c48-4b38-c60b-1decbe14b6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[575.85427001 446.49012138 488.8665109  ...   1.66315922   1.54580234\n",
            "    2.        ]\n",
            " [238.77840725 122.07767083 196.98130991 ...   1.59301201   1.62768437\n",
            "    3.        ]\n",
            " [304.26438833 352.97378125 338.93499076 ...   1.79031258   1.56720704\n",
            "    1.        ]\n",
            " ...\n",
            " [398.93563533 657.26001985 638.48761595 ...   1.86006021   1.92531026\n",
            "    3.        ]\n",
            " [498.7087812  415.99790178 334.25126978 ...   1.76900651   1.67969641\n",
            "    1.        ]\n",
            " [583.36324111 576.99988919 434.06782699 ...   2.00286713   1.72342952\n",
            "    3.        ]]\n",
            "(2388, 61)\n",
            "[[680.1112571  343.37510976 317.60227623 ...   1.72176563   1.4618229\n",
            "    3.        ]\n",
            " [166.05475744 307.72349619 254.62978318 ...   1.62970861   1.25877289\n",
            "    1.        ]\n",
            " [638.68413154 255.332395   399.30067042 ...   1.86896036   1.95694876\n",
            "    4.        ]\n",
            " ...\n",
            " [551.89743738 558.13429394 471.44923553 ...   1.71606381   1.66406737\n",
            "    4.        ]\n",
            " [637.68778122 497.87851938 602.70547796 ...   1.89971066   1.89613421\n",
            "    4.        ]\n",
            " [487.3546205  404.7636359  350.72232041 ...   1.51079863   1.62697326\n",
            "    4.        ]]\n",
            "(341, 61)\n",
            "[[468.750329   518.46511012 420.47265138 ...   1.85274476   1.57440523\n",
            "    1.        ]\n",
            " [696.8971223  331.44597667 306.22313104 ...   1.57763962   1.62200356\n",
            "    3.        ]\n",
            " [456.53391379 437.0528864  570.74465439 ...   1.71721314   1.63965826\n",
            "    4.        ]\n",
            " ...\n",
            " [454.24631909 938.95513227 337.74444131 ...   1.80235446   1.81591508\n",
            "    2.        ]\n",
            " [538.3450476  570.92391022 402.28356465 ...   1.57945617   1.37450674\n",
            "    2.        ]\n",
            " [434.3616972  372.26806379 371.8911643  ...   1.67991659   1.64858147\n",
            "    4.        ]]\n",
            "(683, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we already know, first 851 rows are class label 1, next 855 class label 2, next 855 class label 3 and alst 851 class label 4. We will now create datasets for 1vs2, 1vs3, 1vs4, 2vs3, 2vs4 and 3vs4 respectively."
      ],
      "metadata": {
        "id": "EML7kR2a2O6p"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Q4_NNFL_Assignment1_SoumilHooda.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+IlCawYGmV+I2cCqkiMEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}